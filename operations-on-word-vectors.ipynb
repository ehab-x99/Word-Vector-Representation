{"metadata":{"coursera":{"course_slug":"nlp-sequence-models","graded_item_id":"8hb5s","launcher_item_id":"5NrJ6"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Operations on word vectors\n# Done By: Ehab Osama\n\n**After this mini project I will be able to:**\n\n- Load pre-trained word vectors, and measure similarity using cosine similarity\n- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______. \n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Impoting Libraries ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom w2v_utils import *","metadata":{},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Loading the word vectors\n* For this project, I will use 50-dimensional GloVe vectors to represent words. \n* The following cell loads the `word_to_vec_map`. ","metadata":{}},{"cell_type":"code","source":"words, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"I've loaded:\n- `words`: set of words in the vocabulary.\n- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n","metadata":{}},{"cell_type":"markdown","source":"# 1 - Cosine similarity\n\nTo measure the similarity between two words, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n\n$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n\n* $u \\cdot v$ is the dot product (or inner product) of two vectors\n* $||u||_2$ is the norm (or length) of the vector $u$\n* $\\theta$ is the angle between $u$ and $v$. \n* The cosine similarity depends on the angle between $u$ and $v$. \n    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n    * If they are dissimilar, the cosine similarity will take a smaller value. ","metadata":{}},{"cell_type":"code","source":"#FUNCTION: cosine_similarity\n\ndef cosine_similarity(u, v):\n    \"\"\"\n    Cosine similarity reflects the degree of similarity between u and v\n        \n    Arguments:\n        u -- a word vector of shape (n,)          \n        v -- a word vector of shape (n,)\n\n    Returns:\n        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n    \"\"\"\n    \n    distance = 0.0\n    \n  \n    # Compute the dot product between u and v (≈1 line)\n    dot = np.dot(u,v)\n    # Compute the L2 norm of u (≈1 line)\n    norm_u = np.sqrt(np.sum(u * u))\n    \n    # Compute the L2 norm of v (≈1 line)\n    norm_v = np.sqrt(np.sum(v * v))\n    # Compute the cosine similarity defined by formula (1) (≈1 line)\n    cosine_similarity = dot / (norm_u * norm_v)\n   \n    \n    return cosine_similarity","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"father = word_to_vec_map[\"father\"]\nmother = word_to_vec_map[\"mother\"]\nball = word_to_vec_map[\"ball\"]\ncrocodile = word_to_vec_map[\"crocodile\"]\nfrance = word_to_vec_map[\"france\"]\nitaly = word_to_vec_map[\"italy\"]\nparis = word_to_vec_map[\"paris\"]\nrome = word_to_vec_map[\"rome\"]\n\nprint(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\nprint(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\nprint(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"cosine_similarity(father, mother) =  0.890903844289\n\ncosine_similarity(ball, crocodile) =  0.274392462614\n\ncosine_similarity(france - paris, rome - italy) =  -0.675147930817\n"}]},{"cell_type":"markdown","source":"## 2 - Word analogy task\n\n* In the word analogy task, we complete the sentence:  \n    <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. \n\n* An example is:  \n    <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. \n\n* We are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n    $e_b - e_a \\approx e_d - e_c$\n* We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n\n","metadata":{}},{"cell_type":"code","source":"#FUNCTION: complete_analogy\n\ndef complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n    \"\"\"\n    Performs the word analogy task as explained above: a is to b as c is to ____. \n    \n    Arguments:\n    word_a -- a word, string\n    word_b -- a word, string\n    word_c -- a word, string\n    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n    \n    Returns:\n    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n    \"\"\"\n    \n    # convert words to lowercase\n    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n    \n    \n    # Get the word embeddings e_a, e_b and e_c (≈1-3 lines)\n    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n   \n    \n    words = word_to_vec_map.keys()\n    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n\n    # to avoid best_word being one of the input words, skip the input words\n    # place the input words in a set for faster searching than a list\n    # We will re-use this set of input words inside the for-loop\n    input_words_set = set([word_a, word_b, word_c])\n    \n    # loop over the whole word vector set\n    for w in words:        \n        # to avoid best_word being one of the input words, skip the input words\n        if w in input_words_set:\n            continue\n        \n        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n        \n        # If the cosine_sim is more than the max_cosine_sim seen so far,\n            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n        if cosine_sim > max_cosine_sim:\n            max_cosine_sim = cosine_sim\n            best_word = w\n      \n    return best_word","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The cell below to test the code, this may take 1-2 minutes.","metadata":{}},{"cell_type":"code","source":"triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\nfor triad in triads_to_try:\n    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))","metadata":{},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"italy -> italian :: spain -> spanish\n\nindia -> delhi :: japan -> tokyo\n\nman -> woman :: boy -> girl\n\nsmall -> smaller :: large -> larger\n"}]},{"cell_type":"markdown","source":"**References**:\n- The debiasing algorithm is from Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\nHomemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n- The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n","metadata":{}}]}